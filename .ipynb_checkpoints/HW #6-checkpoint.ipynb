{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #6 \n",
    "\n",
    "1. Find a way to represent the entire time series in a single file. You may either save a visualization for every data on separate pages in a pdf or you may create an MP4 or GIF file. Make sure that for every frame, the correct date is indicated. Differentiate your answer from the template provided in at least 3 ways. Identify how your output is differentiated.\n",
    "\n",
    "2. Select a different time series data (e.g., unemployment, incomes, etc...) set at the county level that has at least 20 observations. Create a map of the latest obervation of the data. Like with the covid data in (1), create a file that holds a visualization for every observation.\n",
    "\n",
    "ECON 611 Only\n",
    "\n",
    "3. Download and plot data relating to the magnitude of the policy response by U.S. state or by all nations across the globe. Plot the data on a map as it changes over time using an MP4 file or a GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-58c596503e3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#createCOVID19StateAndCountyVisualization.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# We won't actually use datetime directly. Since the dataframe index will use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\lib\\site-packages\\geopandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpoints_from_xy\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_read_file\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mread_file\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_read_parquet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mread_parquet\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_read_feather\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mread_feather\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\lib\\site-packages\\geopandas\\io\\file.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyproj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\lib\\site-packages\\fiona\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_gdal_dll_directories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBytesCollection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrvsupport\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msupported_drivers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mensure_env_with_credentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\lib\\site-packages\\fiona\\collection.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_gdal_dll_directories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvfs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mogrext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mItemsIterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKeysIterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mogrext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mogrext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuffer_to_virtual_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_virtual_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGEOMETRY_TYPES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "#createCOVID19StateAndCountyVisualization.py\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# We won't actually use datetime directly. Since the dataframe index will use \n",
    "# data formatted as datetime64, I import it in case I need to use the datetime\n",
    "# module to troubleshoot later \n",
    "import datetime\n",
    "# you could technically call many of the submodules from matplotlib using mpl., \n",
    "#but for convenience we explicitly import submodules. These will be used for \n",
    "# constructing visualizations\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.ticker as mtick\n",
    "import datadotworld as dw\n",
    "\n",
    "\n",
    "def import_geo_data(filename, index_col = \"Date\", FIPS_name = \"FIPS\"):\n",
    "    # import county level shapefile\n",
    "    map_data = geopandas.read_file(filename = filename,                                   \n",
    "                                   index_col = index_col)\n",
    "    # rename fips code to match variable name in COVID-19 data\n",
    "    map_data.rename(columns={\"State\":\"state\"},\n",
    "                    inplace = True)\n",
    "    # Combine statefips and county fips to create a single fips value\n",
    "    # that identifies each particular county without referencing the \n",
    "    # state separately\n",
    "    map_data[FIPS_name] = map_data[\"STATEFP\"].astype(str) + \\\n",
    "        map_data[\"COUNTYFP\"].astype(str)\n",
    "    map_data[FIPS_name] = map_data[FIPS_name].astype(np.int64)\n",
    "    # set FIPS as index\n",
    "    map_data.set_index(FIPS_name, inplace=True)\n",
    "    \n",
    "    return map_data\n",
    "\n",
    "def import_covid_data(filename, FIPS_name):\n",
    "    # Load COVID19 county data using datadotworld API\n",
    "    # Data provided by Johns Hopkins, file provided by Associated Press\n",
    "    dataset = dw.load_dataset(\"associatedpress/johns-hopkins-coronavirus-case-tracker\",\n",
    "                              auto_update = True)\n",
    "    # the dataset includes multiple dataframes. We will only use #2\n",
    "    covid_data = dataset.dataframes[\"2_cases_and_deaths_by_county_timeseries\"]\n",
    "    # Include only oberservation for political entities within states\n",
    "    # i.e., not territories, etc... drop any nan fip values with covid_data[FIPS_name] > 0\n",
    "    covid_data = covid_data[covid_data[FIPS_name] < 57000]\n",
    "    covid_data = covid_data[covid_data[FIPS_name] > 0]\n",
    "\n",
    "    # Transform FIPS codes into integers (not floats)\n",
    "    covid_data[FIPS_name] = covid_data[FIPS_name].astype(int)\n",
    "    covid_data.set_index([FIPS_name, \"date\"], inplace = True)\n",
    "    # Prepare a column for state abbreviations. We will draw these from a\n",
    "    # dictionary created in the next step.\n",
    "    covid_data[\"state_abr\"] = \"\"\n",
    "    for state, abr in state_dict.items():\n",
    "        covid_data.loc[covid_data[\"state\"] == state, \"state_abr\"] = abr\n",
    "    # Create \"Location\" which concatenates county name and state abbreviation \n",
    "    covid_data[\"Location\"] = covid_data[\"location_name\"] + \", \" + \\\n",
    "        covid_data[\"state_abr\"]\n",
    "\n",
    "    return covid_data\n",
    "\n",
    "def create_covid_geo_dataframe(covid_data, map_data, dates):\n",
    "    # create geopandas dataframe with multiindex for date\n",
    "    # original geopandas dataframe had no dates, so copies of the df are \n",
    "    # stacked vertically, with a new copy for each date in the covid_data index\n",
    "    #(dates is a global)\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        # select county observations from each date in dates\n",
    "        df = covid_data[covid_data.index.get_level_values(\"date\")==date]\n",
    "        # use the fips_codes from the slice of covid_data to select counties\n",
    "        # from the map_data index,making sure that the map_data index matches\n",
    "        # the covid_data index\n",
    "        counties = df.index.get_level_values(\"fips_code\")\n",
    "        agg_df = map_data.loc[counties]\n",
    "        # each row for agg_df will reflect that \n",
    "        agg_df[\"date\"] = date\n",
    "        if i == 0:\n",
    "            # create the geodataframe, select coordinate system (.crs) to\n",
    "            # match map_data.crs\n",
    "            matching_gpd = geopandas.GeoDataFrame(agg_df, crs = map_data.crs)\n",
    "            i += 1\n",
    "        else:\n",
    "            # after initial geodataframe is created, stack a dataframe for\n",
    "            # each date in dates. Once completed, index of matching_gpd\n",
    "            # will match index of covid_data\n",
    "            matching_gpd = matching_gpd.append(agg_df, ignore_index = False)         \n",
    "    # Set mathcing_gpd index as[\"fips_code\", \"date\"], liked covid_data index\n",
    "    matching_gpd.reset_index(inplace=True)\n",
    "    matching_gpd.set_index([\"fips_code\",\"date\"], inplace = True)\n",
    "    # add each column from covid_data to mathcing_gpd\n",
    "    for key, val in covid_data.items():\n",
    "        matching_gpd[key] = val\n",
    "\n",
    "    return matching_gpd       \n",
    "\n",
    "def create_new_vars(covid_data, moving_average_days):\n",
    "    # use a for loop that performs the same operations on data for cases and for deaths\n",
    "    for key in [\"cases\", \"deaths\"]:\n",
    "        # create a version of the key with the first letter capitalized\n",
    "        cap_key = key.title()\n",
    "        covid_data[cap_key + \" per Million\"] = covid_data[\"cumulative_\" + key]\\\n",
    "            .div(covid_data[\"total_population\"]).mul(10 ** 6)\n",
    "        # generate daily data normalized per million population by taking the daily difference within each\n",
    "        # entity (covid_data.index.names[0]), dividing this value by population and multiplying that value\n",
    "        # by 1 million 10 ** 6\n",
    "        covid_data[\"Daily \" + cap_key + \" per Million\"] = \\\n",
    "            covid_data[\"cumulative_\" + key ].groupby(covid_data.index.names[0])\\\n",
    "            .diff(1).div(covid_data[\"total_population\"]).mul(10 ** 6)\n",
    "        # taking the rolling average; choice of number of days is passed as moving_average_days\n",
    "        covid_data[\"Daily \" + cap_key + \" per Million MA\"] = covid_data[\"Daily \" + \\\n",
    "                  cap_key + \" per Million\"].rolling(moving_average_days).mean()\n",
    "\n",
    "# I include this dictionary to convenienlty cross reference state names and\n",
    "# state abbreviations.\n",
    "state_dict = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', \n",
    "    'Delaware': 'DE', 'District of Columbia': 'DC', 'Florida': 'FL', \n",
    "    'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL',\n",
    "    'Indiana': 'IN', 'Iowa': 'IA','Kansas': 'KS', 'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n",
    "    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX',\n",
    "    'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA',\n",
    "    'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'}\n",
    "\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams.update({'font.size': 32})\n",
    "\n",
    "if \"data_processed\" not in locals():\n",
    "    fips_name = \"fips_code\"\n",
    "    covid_filename = \"COVID19DataAP.csv\"\n",
    "    # rename_FIPS matches map_data FIPS with COVID19 FIPS name\n",
    "    map_data = import_geo_data(filename = \"countiesWithStatesAndPopulation.shp\",\n",
    "                    index_col = \"Date\", FIPS_name= fips_name)\n",
    "    covid_data = import_covid_data(filename = covid_filename, FIPS_name = fips_name)\n",
    "    # dates will be used to create a geopandas DataFrame with multiindex \n",
    "    dates = sorted(list(set(covid_data.index.get_level_values(\"date\"))))\n",
    "    covid_data = create_covid_geo_dataframe(covid_data, map_data, dates)\n",
    "    moving_average_days = 7\n",
    "    create_new_vars(covid_data, moving_average_days)\n",
    "    start_date = \"03-15-2020\"     \n",
    "    end_date = dates[-1]\n",
    "    # once data is processed, it is saved in the memory\n",
    "    # the if statement at the top of this block of code instructs the computer\n",
    "    # not to repeat these operations \n",
    "    data_processed = True\n",
    "    \n",
    "pp = PdfPages('Project#6.pdf')\n",
    "pp.savefig(map_data)\n",
    "pp.savefig(covid_data)\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1dfb4872e050>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"map_bounded\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dates' is not defined"
     ]
    }
   ],
   "source": [
    "def select_data_within_bounds(data, minx, miny, maxx, maxy):\n",
    "    data = data[data.bounds[\"maxx\"] <= maxx]\n",
    "    data = data[data.bounds[\"maxy\"] <= maxy]\n",
    "    data = data[data.bounds[\"minx\"] >= minx]\n",
    "    data = data[data.bounds[\"miny\"] >= miny]\n",
    "    \n",
    "    return data\n",
    "\n",
    "date = dates[-1]\n",
    "\n",
    "if \"map_bounded\" not in locals():\n",
    "    minx = -127\n",
    "    miny = 23\n",
    "    maxx = -58\n",
    "    maxy = 54\n",
    "    covid_map_data = select_data_within_bounds(covid_data, minx, miny, maxx, maxy)\n",
    "    map_bounded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fbceaabfd9e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m fig, ax = plt.subplots(figsize=(18,8),\n\u001b[0m\u001b[0;32m      2\u001b[0m         subplot_kw = {'aspect': 'equal'})   \n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"font.size\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfontsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfontsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,8),\n",
    "        subplot_kw = {'aspect': 'equal'})   \n",
    "plt.rcParams.update({\"font.size\": 30})\n",
    "plt.xticks(fontsize = 25)\n",
    "plt.yticks(fontsize = 25)\n",
    "key = \"Deaths per Million\"\n",
    "# this time we replace 0 values with 1\n",
    "# so that these values show up as beige instead of as white\n",
    "# when color axis is logged\n",
    "df = covid_map_data[covid_map_data.index.get_level_values(\"date\")==date].replace(0,1)\n",
    "# set range of colorbar\n",
    "vmin = 1 \n",
    "vmax = df[key].max()\n",
    "# choose colormap\n",
    "cmap = cm.get_cmap('Reds', 4)\n",
    "# format colormap\n",
    "norm = cm.colors.LogNorm(vmin = vmin, vmax = vmax)\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "# prepare space for colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "size = \"5%\" \n",
    "cax = divider.append_axes(\"right\", size = size, pad = 0.1)\n",
    "# add colorbar to figure\n",
    "cbar = fig.colorbar(sm, cax=cax, cmap = cmap)\n",
    "cbar.ax.tick_params(labelsize=18)\n",
    "vals = list(cbar.ax.get_yticks())\n",
    "vals.append(vmax)\n",
    "# format colorbar values as int\n",
    "cbar.ax.set_yticklabels([int(x) for x in vals])\n",
    "cbar.ax.set_ylabel(key, fontsize = 20)\n",
    "\n",
    "\n",
    "df.plot(ax=ax, cax = cax, column=key, vmin=vmin ,vmax = vmax, \n",
    "             cmap = cmap, legend=False, linewidth=.5, edgecolor='lightgrey', \n",
    "             norm = norm)\n",
    "ax.set_title(str(date)[:10] + \"\\n\" + \"COVID-19 in the U.S.\", fontsize = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fbceaabfd9e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m fig, ax = plt.subplots(figsize=(18,8),\n\u001b[0m\u001b[0;32m      2\u001b[0m         subplot_kw = {'aspect': 'equal'})   \n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"font.size\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfontsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfontsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8),\n",
    "        subplot_kw = {'aspect': 'equal'})   \n",
    "plt.rcParams.update({\"font.size\": 30})\n",
    "plt.xticks(fontsize = 25)\n",
    "plt.yticks(fontsize = 25)\n",
    "key = \"Deaths per Million\"\n",
    "# this time we replace 0 values with 1\n",
    "# so that these values show up as beige instead of as white\n",
    "# when color axis is logged\n",
    "df = covid_map_data[covid_map_data.index.get_level_values(\"date\")==date].replace(0,1)\n",
    "# set range of colorbar\n",
    "vmin = 1 \n",
    "vmax = df[key].max()\n",
    "# choose colormap\n",
    "cmap = cm.get_cmap('Reds', 4)\n",
    "# format colormap\n",
    "norm = cm.colors.LogNorm(vmin = vmin, vmax = vmax)\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "# prepare space for colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "size = \"5%\" \n",
    "cax = divider.append_axes(\"right\", size = size, pad = 0.1)\n",
    "# add colorbar to figure\n",
    "cbar = fig.colorbar(sm, cax=cax, cmap = cmap)\n",
    "cbar.ax.tick_params(labelsize=18)\n",
    "vals = list(cbar.ax.get_yticks())\n",
    "vals.append(vmax)\n",
    "# format colorbar values as int\n",
    "cbar.ax.set_yticklabels([int(x) for x in vals])\n",
    "cbar.ax.set_ylabel(key, fontsize = 20)\n",
    "\n",
    "\n",
    "df.plot(ax=ax, cax = cax, column=key, vmin=vmin ,vmax = vmax, \n",
    "             cmap = cmap, legend=False, linewidth=.5, edgecolor='lightgrey', \n",
    "             norm = norm)\n",
    "ax.set_title(str(date)[:10] + \"\\n\" + \"COVID-19 in the U.S.\", fontsize = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_geo_data(filename, index_col = \"Date\", FIPS_name = \"FIPS\"):\n",
    "    # import county level shapefile\n",
    "    map_data = geopandas.read_file(filename = filename,                                   \n",
    "                                   index_col = index_col)\n",
    "    # rename fips code to match variable name in COVID-19 data\n",
    "    map_data.rename(columns={\"State\":\"state\"},\n",
    "                    inplace = True)\n",
    "    # Combine statefips and county fips to create a single fips value\n",
    "    # that identifies each particular county without referencing the \n",
    "    # state separately\n",
    "    map_data[FIPS_name] = map_data[\"STATEFP\"].astype(str) + \\\n",
    "        map_data[\"COUNTYFP\"].astype(str)\n",
    "    map_data[FIPS_name] = map_data[FIPS_name].astype(np.int64)\n",
    "    # set FIPS as index\n",
    "    map_data.set_index(FIPS_name, inplace=True)\n",
    "    \n",
    "    return map_data\n",
    "\n",
    "\n",
    "def import_Umemployment_data(filename, FIPS_name):\n",
    "    # data provided by USDA\n",
    "    # https://www.ers.usda.gov/data-products/county-level-data-sets/download-data/\n",
    "    dataset = dw.load_dataset(\"unemployment.csv\",\n",
    "                              auto_update = True)\n",
    "    # the dataset includes multiple dataframes. We will only use\n",
    "    unemployment_data = dataset.dataframes[\"Unemployment_rate_2019\"]\n",
    "    # drop any nan fip values with covid_data[FIPS_name] > 0\n",
    "    unemployment_data = unemployment_data[covid_data[FIPS_name] < 57000]\n",
    "    unemployment_data = unemployment_data[covid_data[FIPS_name] > 0]\n",
    "\n",
    "    # Transform FIPS codes into integers (not floats)\n",
    "    unemployment_data[FIPS_name] = unemployment_data[FIPS_name].astype(int)\n",
    "    unemployment_data.set_index([FIPS_name, \"date\"], inplace = True)\n",
    "    # Prepare a column for state abbreviations. We will draw these from a\n",
    "    # dictionary created in the next step.\n",
    "    unemployment_data[\"state_abr\"] = \"\"\n",
    "    for state, abr in state_dict.items():\n",
    "        unemployment_data.loc[unemployment_data[\"state\"] == state, \"state_abr\"] = abr\n",
    "    # Create \"Location\" which concatenates county name and state abbreviation \n",
    "    unemployment_data[\"Location\"] = unemployment_data[\"location_name\"] + \", \" + \\\n",
    "        unemployment_data[\"state_abr\"]\n",
    "\n",
    "    return unemployment_data\n",
    "\n",
    "def create_unemployment_geo_dataframe(unemployment_data, map_data, dates):\n",
    "    # create geopandas dataframe with multiindex for date\n",
    "    # original geopandas dataframe had no dates, so copies of the df are \n",
    "    # stacked vertically, with a new copy for each date in the covid_data index\n",
    "    #(dates is a global)\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        # select county observations from each date in dates\n",
    "        df = unemployment_data[unemployment_data.index.get_level_values(\"date\")==date]\n",
    "        # use the fips_codes from the slice of covid_data to select counties\n",
    "        # from the map_data index,making sure that the map_data index matches\n",
    "        # the covid_data index\n",
    "        counties = df.index.get_level_values(\"fips_code\")\n",
    "        agg_df = map_data.loc[counties]\n",
    "        # each row for agg_df will reflect that \n",
    "        agg_df[\"date\"] = date\n",
    "        if i == 0:\n",
    "            # create the geodataframe, select coordinate system (.crs) to\n",
    "            # match map_data.crs\n",
    "            matching_gpd = geopandas.GeoDataFrame(agg_df, crs = map_data.crs)\n",
    "            i += 1\n",
    "        else:\n",
    "            # after initial geodataframe is created, stack a dataframe for\n",
    "            # each date in dates. Once completed, index of matching_gpd\n",
    "            # will match index of covid_data\n",
    "            matching_gpd = matching_gpd.append(agg_df, ignore_index = False)         \n",
    "    # Set mathcing_gpd index as[\"fips_code\", \"date\"], liked covid_data index\n",
    "    matching_gpd.reset_index(inplace=True)\n",
    "    matching_gpd.set_index([\"fips_code\",\"date\"], inplace = True)\n",
    "    # add each column from covid_data to mathcing_gpd\n",
    "    for key, val in covid_data.items():\n",
    "        matching_gpd[key] = val\n",
    "\n",
    "    return matching_gpd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_Umemployment_data(filename, FIPS_name):\n",
    "    dataset = dw.load_dataset(\"OxCGRT_latest.csv\",\n",
    "                              auto_update = True)\n",
    "    # the dataset includes multiple dataframes. We will only use\n",
    "    unemployment_data = dataset.dataframes[\"Unemployment_rate_2019\"]\n",
    "    # drop any nan fip values with covid_data[FIPS_name] > 0\n",
    "    unemployment_data = unemployment_data[covid_data[FIPS_name] < 57000]\n",
    "    unemployment_data = unemployment_data[covid_data[FIPS_name] > 0]\n",
    "\n",
    "    # Transform FIPS codes into integers (not floats)\n",
    "    unemployment_data[FIPS_name] = unemployment_data[FIPS_name].astype(int)\n",
    "    unemployment_data.set_index([FIPS_name, \"date\"], inplace = True)\n",
    "    # Prepare a column for state abbreviations. We will draw these from a\n",
    "    # dictionary created in the next step.\n",
    "    unemployment_data[\"state_abr\"] = \"\"\n",
    "    for state, abr in state_dict.items():\n",
    "        unemployment_data.loc[unemployment_data[\"state\"] == state, \"state_abr\"] = abr\n",
    "    # Create \"Location\" which concatenates county name and state abbreviation \n",
    "    unemployment_data[\"Location\"] = unemployment_data[\"location_name\"] + \", \" + \\\n",
    "        unemployment_data[\"state_abr\"]\n",
    "\n",
    "    return unemployment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datareader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-99810e353f4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_Policy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatareader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOxCGRT_latest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# set range of colorbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# choose colormap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datareader' is not defined"
     ]
    }
   ],
   "source": [
    "df_Policy = datareader(OxCGRT_latest.csv)\n",
    "# set range of colorbar\n",
    "vmin = 1 \n",
    "vmax = df[key].max()\n",
    "# choose colormap\n",
    "cmap = cm.get_cmap('Blue', 4)\n",
    "# format colormap\n",
    "norm = cm.colors.LogNorm(vmin = vmin, vmax = vmax)\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "# prepare space for colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "size = \"5%\" \n",
    "cax = divider.append_axes(\"right\", size = size, pad = 0.1)\n",
    "# add colorbar to figure\n",
    "cbar = fig.colorbar(sm, cax=cax, cmap = cmap)\n",
    "cbar.ax.tick_params(labelsize=18)\n",
    "vals = list(cbar.ax.get_yticks())\n",
    "vals.append(vmax)\n",
    "# format colorbar values as int\n",
    "cbar.ax.set_yticklabels([int(x) for x in vals])\n",
    "cbar.ax.set_ylabel(key, fontsize = 20)\n",
    "\n",
    "\n",
    "df.plot(ax=ax, cax = cax, column=key, vmin=vmin ,vmax = vmax, \n",
    "             cmap = cmap, legend=False, linewidth=.5, edgecolor='lightgrey', \n",
    "             norm = norm)\n",
    "ax.set_title(str(date)[:10] + \"\\n\" + \"C1_flag\", fontsize = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"map_bounded\" not in locals():\n",
    "    minx = -127\n",
    "    miny = 23\n",
    "    maxx = -58\n",
    "    maxy = 54\n",
    "    covid_data = select_data_within_bounds(covid_data, minx, miny, maxx, maxy)\n",
    "    map_bounded = True\n",
    "\n",
    "\n",
    "for key in keys:\n",
    "    log = False if \"Daily\" in key else True\n",
    "    # this time we replace 0 values with 1\n",
    "    # so that these values show up as beige  instead of as white\n",
    "    # when color axis is logged\n",
    "    vmin = 1 if log else 0 \n",
    "    vmax = df[key][df.index.get_level_values(\"date\") == date].max()\n",
    "    # Create colorbar as a legend\n",
    "    cmap = cm.get_cmap('Reds', 4)\n",
    "    if log:\n",
    "        norm = cm.colors.LogNorm(vmin=vmin, vmax =vmax)\n",
    "    else:\n",
    "        norm = cm.colors.Normalize(vmin = vmin, vmax = vmax)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(18,8),\n",
    "        subplot_kw = {'aspect': 'equal'})   \n",
    "    plt.rcParams.update({\"font.size\": 30})\n",
    "    plt.xticks(fontsize = 25)\n",
    "    plt.yticks(fontsize = 25)\n",
    "    # the functions will unpack the tuple. The same names variable names\n",
    "    # are used in the function\n",
    "    kwargs = (df, key, log, date, fig, ax, cmap, norm, vmin, vmax)\n",
    "    init(kwargs)\n",
    "    plot_map(kwargs)\n",
    "    \n",
    "    plt.show()    \n",
    "\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
